{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import of necessary modules and packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.drawing.nx_pydot import write_dot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from IPython.display import display_png\n",
    "import json\n",
    "from pathlib import Path\n",
    "import scipy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import dgl\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from karateclub.node_embedding.neighbourhood.deepwalk import DeepWalk\n",
    "from nodevectors import Node2Vec\n",
    "from gensim.models import Word2Vec\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training preprocessing done\n",
      "test preprocessing done\n"
     ]
    }
   ],
   "source": [
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "def load_data(transcription_id, labels, data_type='training'):\n",
    "    with open(f'{data_type}/{transcription_id}.json', 'r') as json_file:\n",
    "        discourse_data = json.load(json_file)\n",
    "\n",
    "    with open(f'{data_type}/{transcription_id}.txt', 'r') as txt_file:\n",
    "        discourse_types = [line.strip().split() for line in txt_file]\n",
    "\n",
    "    G = nx.DiGraph()\n",
    "    for entry in discourse_data:\n",
    "        G.add_node(entry['index'], speaker=entry['speaker'], text=entry['text'])\n",
    "\n",
    "    for discourse_relation in discourse_types:\n",
    "        node_from = int(discourse_relation[0])\n",
    "        node_to = int(discourse_relation[-1])\n",
    "        relation_type = discourse_relation[1]\n",
    "\n",
    "        if G.has_node(node_from) and G.has_node(node_to):\n",
    "            G.add_edge(node_from, node_to, type=relation_type)\n",
    "\n",
    "    label = labels[transcription_id]\n",
    "    \n",
    "    return G, label\n",
    "\n",
    "def precompute_embeddings(bert, data):\n",
    "    embeddings = []\n",
    "    for G, _ in data:\n",
    "        deepwalk = DeepWalk()\n",
    "        deepwalk.fit(G)\n",
    "        embedding = deepwalk.get_embedding()\n",
    "        embeddings.append(embedding)\n",
    "    return flatten(embeddings)\n",
    "\n",
    "# training and test sets of transcription ids\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "training_set, test_set = train_test_split(training_set, test_size=0.2, random_state=42)\n",
    "\n",
    "with open(\"training_labels.json\", \"r\") as file:\n",
    "    training_labels = json.load(file)\n",
    "\n",
    "bert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# training graph preprocessing\n",
    "y_training = []\n",
    "X_training_data = [load_data(transcription_id, training_labels) for transcription_id in training_set]\n",
    "X_training_graphs = [G for G, _ in X_training_data]\n",
    "y_training = flatten([label for _, label in X_training_data])\n",
    "X_training = precompute_embeddings(bert, X_training_data)\n",
    "print('training preprocessing done')\n",
    "\n",
    "# test graph preprocessing\n",
    "y_test = []\n",
    "X_test_data = [load_data(transcription_id, training_labels, data_type='training') for transcription_id in test_set]\n",
    "y_test = flatten([label for _, label in X_test_data])\n",
    "X_test_graphs = [G for G, _ in X_test_data]\n",
    "X_test = precompute_embeddings(bert, X_test_data)\n",
    "print('test preprocessing done')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model training and comparison with \"pure\" discourse graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.20256"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_training, y_training)\n",
    "print('training done')\n",
    "\n",
    "# test\n",
    "y_pred = clf.predict(X_test)\n",
    "f1_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.01000667111407605"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "model = XGBClassifier(random_state=0)\n",
    "model.fit(X_training, y_training)\n",
    "print('training done')\n",
    "\n",
    "# test\n",
    "y_pred2 = model.predict(X_test)\n",
    "f1_score(y_test, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model training\n",
    "model = RandomForestClassifier(random_state=0)\n",
    "model.fit(X_training, y_training)\n",
    "print('training done')\n",
    "\n",
    "# test\n",
    "y_pred3 = model.predict(X_test)\n",
    "\n",
    "f1_score(y_test, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Number of positive: 10374, number of negative: 46996\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.033284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 32640\n",
      "[LightGBM] [Info] Number of data points in the train set: 57370, number of used features: 128\n",
      "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.180826 -> initscore=-1.510760\n",
      "[LightGBM] [Info] Start training from score -1.510760\n",
      "training done\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier\n",
    "\n",
    "# model training\n",
    "model = LGBMClassifier(random_state=0)\n",
    "model.fit(X_training, y_training)\n",
    "print('training done')\n",
    "\n",
    "# test\n",
    "y_pred_LGBM = model.predict(X_test)\n",
    "\n",
    "f1_score(y_test, y_pred_LGBM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attempts to use DGL graphs (unsuccessful debugging)\n",
    "\n",
    "- From the existing networkx graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'is_directed'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\lelia\\Documents\\0_X3A\\INF554 - Apprentissage automatique et profond\\inf554-extractive-summarization-2023\\extract_sum_code[1].ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m graphs\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Create DGL graphs and add node features\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m train_graphs \u001b[39m=\u001b[39m create_dgl_graphs(X_training)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m test_graphs \u001b[39m=\u001b[39m create_dgl_graphs(X_test)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Define the GCN model\u001b[39;00m\n",
      "\u001b[1;32mc:\\Users\\lelia\\Documents\\0_X3A\\INF554 - Apprentissage automatique et profond\\inf554-extractive-summarization-2023\\extract_sum_code[1].ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_dgl_graphs\u001b[39m(data):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     graphs \u001b[39m=\u001b[39m [dgl\u001b[39m.\u001b[39mfrom_networkx(G) \u001b[39mfor\u001b[39;00m G \u001b[39min\u001b[39;00m data]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m graphs:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# Extract 'speaker' and 'text' attributes\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         speakers \u001b[39m=\u001b[39m [g\u001b[39m.\u001b[39mnodes[n][\u001b[39m'\u001b[39m\u001b[39mspeaker\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m g\u001b[39m.\u001b[39mnodes()]\n",
      "\u001b[1;32mc:\\Users\\lelia\\Documents\\0_X3A\\INF554 - Apprentissage automatique et profond\\inf554-extractive-summarization-2023\\extract_sum_code[1].ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate_dgl_graphs\u001b[39m(data):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     graphs \u001b[39m=\u001b[39m [dgl\u001b[39m.\u001b[39;49mfrom_networkx(G) \u001b[39mfor\u001b[39;00m G \u001b[39min\u001b[39;00m data]\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39mfor\u001b[39;00m g \u001b[39min\u001b[39;00m graphs:\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m         \u001b[39m# Extract 'speaker' and 'text' attributes\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/lelia/Documents/0_X3A/INF554%20-%20Apprentissage%20automatique%20et%20profond/inf554-extractive-summarization-2023/extract_sum_code%5B1%5D.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m         speakers \u001b[39m=\u001b[39m [g\u001b[39m.\u001b[39mnodes[n][\u001b[39m'\u001b[39m\u001b[39mspeaker\u001b[39m\u001b[39m'\u001b[39m] \u001b[39mfor\u001b[39;00m n \u001b[39min\u001b[39;00m g\u001b[39m.\u001b[39mnodes()]\n",
      "File \u001b[1;32mc:\\Users\\lelia\\anaconda3\\lib\\site-packages\\dgl\\convert.py:1344\u001b[0m, in \u001b[0;36mfrom_networkx\u001b[1;34m(nx_graph, node_attrs, edge_attrs, edge_id_attr_name, idtype, device)\u001b[0m\n\u001b[0;32m   1335\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1336\u001b[0m     edge_id_attr_name \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1337\u001b[0m     \u001b[39mand\u001b[39;00m edge_id_attr_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mnext\u001b[39m(\u001b[39miter\u001b[39m(nx_graph\u001b[39m.\u001b[39medges(data\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)))[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m   1338\u001b[0m ):\n\u001b[0;32m   1339\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\n\u001b[0;32m   1340\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFailed to find the pre-specified edge IDs in the edge features of \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1341\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mthe NetworkX graph with name \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(edge_id_attr_name)\n\u001b[0;32m   1342\u001b[0m     )\n\u001b[1;32m-> 1344\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m nx_graph\u001b[39m.\u001b[39;49mis_directed() \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[0;32m   1345\u001b[0m     edge_id_attr_name \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m edge_attrs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1346\u001b[0m ):\n\u001b[0;32m   1347\u001b[0m     \u001b[39mraise\u001b[39;00m DGLError(\n\u001b[0;32m   1348\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mExpect edge_id_attr_name and edge_attrs to be None when nx_graph is \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1349\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mundirected, got \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m and \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(edge_id_attr_name, edge_attrs)\n\u001b[0;32m   1350\u001b[0m     )\n\u001b[0;32m   1352\u001b[0m \u001b[39m# Relabel nodes using consecutive integers starting from 0\u001b[39;00m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'is_directed'"
     ]
    }
   ],
   "source": [
    "# Function to create DGL graphs and add node features\n",
    "def create_dgl_graphs(data):\n",
    "    graphs = [dgl.from_networkx(G) for G in data]\n",
    "\n",
    "    for g in graphs:\n",
    "        # Extract 'speaker' and 'text' attributes\n",
    "        speakers = [g.nodes[n]['speaker'] for n in g.nodes()]\n",
    "        texts = [g.nodes[n]['text'] for n in g.nodes()]\n",
    "\n",
    "        # Encode 'speaker' and 'text' using OneHotEncoder\n",
    "        encoder = OneHotEncoder(sparse=False, handle_unknown='ignore')\n",
    "        speakers_encoded = encoder.fit_transform([[s] for s in speakers])\n",
    "        texts_encoded = encoder.fit_transform([[t] for t in texts])\n",
    "\n",
    "        # Add one-hot encoded features to the node data of DGL graphs\n",
    "        g.ndata['speaker'] = torch.tensor(speakers_encoded).float()\n",
    "        g.ndata['text'] = torch.tensor(texts_encoded).float()\n",
    "\n",
    "    return graphs\n",
    "\n",
    "# Create DGL graphs and add node features\n",
    "train_graphs = create_dgl_graphs(X_training)\n",
    "test_graphs = create_dgl_graphs(X_test)\n",
    "\n",
    "# Define the GCN model\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_feats, hidden_size, num_classes):\n",
    "        super(GCN, self).__init__()\n",
    "        self.conv1 = dgl.nn.GraphConv(in_feats, hidden_size)\n",
    "        self.conv2 = dgl.nn.GraphConv(hidden_size, hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, g):\n",
    "        x = F.relu(self.conv1(g, g.ndata['speaker']))\n",
    "        x = F.relu(self.conv2(g, x))\n",
    "        x = dgl.mean_nodes(g, 'x')\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "# Create the GCN model\n",
    "in_feats = train_graphs[0].ndata['speaker'].shape[1] + train_graphs[0].ndata['text'].shape[1]  # Number of input features\n",
    "hidden_size = 64  # Number of hidden units\n",
    "num_classes = 2  # Number of output classes\n",
    "model = GCN(in_feats, hidden_size, num_classes)\n",
    "\n",
    "# Convert the training labels to PyTorch tensors\n",
    "y_train_tensor = torch.tensor(y_train)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = [model(g) for g in train_graphs]\n",
    "    loss = sum([criterion(output, y) for output, y in zip(outputs, y_train_tensor)])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = [model(g) for g in test_graphs]\n",
    "    y_pred = torch.argmax(torch.cat(test_outputs), dim=1)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "print('F1 score:', f1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- from scratch extracting the data from the json and txt files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import os\n",
    "from dgl import DGLGraph\n",
    "import dgl.function as fn\n",
    "\n",
    "# Utility function to flatten list of lists\n",
    "def flatten(list_of_list):\n",
    "    return [item for sublist in list_of_list for item in sublist]\n",
    "\n",
    "# Load the training labels from a json file\n",
    "with open(\"training_labels.json\", \"r\") as file:\n",
    "    training_labels = json.load(file)\n",
    "\n",
    "# Load the data for a single transcription and its corresponding label list\n",
    "def load_data(transcription_id, label_list, data_type='training'):\n",
    "    # Load the discourse data and types from separate json and txt files\n",
    "    with open(f'{data_type}/{transcription_id}.json', 'r') as json_file:\n",
    "        discourse_data = json.load(json_file)\n",
    "\n",
    "    with open(f'{data_type}/{transcription_id}.txt', 'r') as txt_file:\n",
    "        discourse_types = [line.strip().split() for line in txt_file]\n",
    "        \n",
    "    # Load the corresponding labels from the label list\n",
    "    labels = label_list[transcription_id]\n",
    "\n",
    "    # Create a DGLGraph\n",
    "    G = DGLGraph()\n",
    "\n",
    "    # Add nodes to the graph with the speaker, text, and label attributes\n",
    "    for i in range(len(discourse_data)):\n",
    "        entry = discourse_data[i]\n",
    "        node_id = entry['index']\n",
    "        speaker = entry['speaker']\n",
    "        text = entry['text']\n",
    "        label = labels[i]\n",
    "\n",
    "        G.add_nodes(1, {'speaker': torch.tensor(speaker), 'text': torch.tensor(text), 'label': torch.tensor(label)})\n",
    "\n",
    "    # Add edges to the graph with the relation type attribute\n",
    "    for discourse_relation in discourse_types:\n",
    "        node_from = int(discourse_relation[0])\n",
    "        node_to = int(discourse_relation[-1])\n",
    "        relation_type = discourse_relation[1]\n",
    "\n",
    "        G.add_edges([node_from], [node_to], {'relation': relation_type})\n",
    "\n",
    "    return G\n",
    "\n",
    "# Load the data for a list of transcriptions and their corresponding label list\n",
    "def process_data(transcription_ids, label_list, data_type='training'):\n",
    "    graphs = []\n",
    "    for transcription_id in transcription_ids:\n",
    "        graph = load_data(transcription_id, label_list, data_type)\n",
    "        graphs.append(graph)\n",
    "    return graphs\n",
    "\n",
    "# Create training and test sets of transcription ids\n",
    "training_set = ['ES2002', 'ES2005', 'ES2006', 'ES2007', 'ES2008', 'ES2009', 'ES2010', 'ES2012', 'ES2013', 'ES2015', 'ES2016', 'IS1000', 'IS1001', 'IS1002', 'IS1003', 'IS1004', 'IS1005', 'IS1006', 'IS1007', 'TS3005', 'TS3008', 'TS3009', 'TS3010', 'TS3011', 'TS3012']\n",
    "training_set = flatten([[m_id+s_id for s_id in 'abcd'] for m_id in training_set])\n",
    "training_set.remove('IS1002a')\n",
    "training_set.remove('IS1005d')\n",
    "training_set.remove('TS3012c')\n",
    "\n",
    "training_set, test_set = train_test_split(training_set, test_size=0.2, random_state=42)\n",
    "\n",
    "# Process the training set\n",
    "X_training = process_data(training_set, training_labels)\n",
    "\n",
    "# Process the test set\n",
    "X_test = process_data(test_set, training_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
